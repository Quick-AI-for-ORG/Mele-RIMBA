{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parse the text file for labeled segments\n",
    "def parse_text_file(file_path):\n",
    "    segments = []\n",
    "    def is_float(value):\n",
    "        \"\"\"Check if a string can be converted to a float.\"\"\"\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()[1:]  # Skip the first line (header), if needed\n",
    "        \n",
    "        for line in lines:\n",
    "            # Strip leading/trailing spaces and check if the line is empty\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  \n",
    "            \n",
    "            # Split the line into parts (by spaces or tabs)\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Check if there are exactly three parts and if the first two parts are floats\n",
    "            if len(parts) == 3 and is_float(parts[0]) and is_float(parts[1]):\n",
    "                start, end, label = float(parts[0]), float(parts[1]), parts[2]\n",
    "                segments.append((start, end, label))\n",
    "            else:\n",
    "                print(f\"Skipping line due to incorrect format: {line}\")\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract corresponding audio segments\n",
    "def extract_audio_segments(audio_path, segments):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    audio_segments = []\n",
    "    labels = []\n",
    "    \n",
    "    for start, end, label in segments:\n",
    "        start_sample = int(start * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        segment = y[start_sample:end_sample]\n",
    "        audio_segments.append(segment)\n",
    "        labels.append(1 if label == \"bee\" else 0)  # 1 for \"bee\", 0 for \"nobee\"\n",
    "    \n",
    "    return audio_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess audio segments (MFCC feature extraction)\n",
    "def preprocess_audio_segments(audio_segments, sr):\n",
    "    mfcc_features = []\n",
    "    \n",
    "    for segment in audio_segments:\n",
    "        # Extract MFCC features for each segment\n",
    "        mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=40)\n",
    "        mfcc_features.append(mfcc)\n",
    "    \n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Pad audio features to ensure equal length\n",
    "def pad_audio_segments(mfcc_features):\n",
    "    # Find the maximum length (number of time steps) across all MFCC segments\n",
    "    max_length = max([mfcc.shape[1] for mfcc in mfcc_features])\n",
    "    padded_mfccs = []\n",
    "    \n",
    "    for mfcc in mfcc_features:\n",
    "        # Pad with zeros to ensure all segments have the same number of time steps\n",
    "        pad_width = max_length - mfcc.shape[1]\n",
    "        if pad_width > 0:\n",
    "            # Pad only along the time steps axis (second dimension)\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        padded_mfccs.append(mfcc)\n",
    "    \n",
    "    return np.array(padded_mfccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a CNN model for binary classification\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line due to incorrect format: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.7500 - loss: 0.4194 - val_accuracy: 1.0000 - val_loss: 0.3117\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0576\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 1.0000 - loss: 4.6698e-04 - val_accuracy: 1.0000 - val_loss: 0.0128\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 1.0000 - loss: 3.7098e-05 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 1.0000 - loss: 4.0653e-06 - val_accuracy: 1.0000 - val_loss: 8.8012e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 1.0000 - loss: 6.4584e-07 - val_accuracy: 1.0000 - val_loss: 2.7412e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 1.1480e-07 - val_accuracy: 1.0000 - val_loss: 8.7403e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 2.2190e-08 - val_accuracy: 1.0000 - val_loss: 3.1430e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 1.0000 - loss: 4.7058e-09 - val_accuracy: 1.0000 - val_loss: 1.2082e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 1.0000 - loss: 1.1081e-09 - val_accuracy: 1.0000 - val_loss: 4.8500e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b08e2de860>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = 'CF003 - Active - Day - (216).wav'  \n",
    "text_file_path = 'CF003 - Active - Day - (216).lab'\n",
    "\n",
    "# Step 1: Parse the text file for labeled segments\n",
    "segments = parse_text_file(text_file_path)\n",
    "\n",
    "# Step 2: Extract corresponding audio segments\n",
    "audio_segments, labels = extract_audio_segments(audio_path, segments)\n",
    "\n",
    "# Step 3: Preprocess audio segments (MFCC feature extraction)\n",
    "sr = 22050  # Sampling rate, librosa defaults to 22050\n",
    "mfcc_features = preprocess_audio_segments(audio_segments, sr)\n",
    "\n",
    "# Step 4: Pad audio features to ensure equal length\n",
    "padded_mfcc_features = pad_audio_segments(mfcc_features)\n",
    "\n",
    "X = np.array(padded_mfcc_features)\n",
    "y = np.array(labels)\n",
    "\n",
    "input_shape = (X.shape[1], X.shape[2])  # (time steps, MFCC coefficients)\n",
    "model = create_model(input_shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line due to incorrect format: .\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 9.7023e-07\n",
      "Test Loss: 9.70233372754592e-07, Test Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 9.7023e-07\n",
      "Test Loss: 9.70233372754592e-07, Test Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "Audio segment 1: Bee detected with confidence 1.00\n",
      "Audio segment 2: No bee detected with confidence 1.00\n",
      "Audio segment 3: Bee detected with confidence 1.00\n",
      "Audio segment 4: No bee detected with confidence 1.00\n",
      "Audio segment 5: Bee detected with confidence 1.00\n"
     ]
    }
   ],
   "source": [
    "test_audio_path = 'CF003 - Active - Day - (216).wav'  \n",
    "test_text_file_path = 'CF003 - Active - Day - (216).lab' \n",
    "\n",
    "# Step 1: Parse the text file for labeled segments\n",
    "test_segments = parse_text_file(test_text_file_path)\n",
    "\n",
    "# Step 2: Extract corresponding audio segments\n",
    "test_audio_segments, test_labels = extract_audio_segments(test_audio_path, test_segments)\n",
    "\n",
    "# Step 3: Preprocess test audio segments (MFCC feature extraction)\n",
    "test_mfcc_features = preprocess_audio_segments(test_audio_segments, sr=22050)\n",
    "\n",
    "# Step 4: Pad test audio features to ensure equal length\n",
    "padded_test_mfcc_features = pad_audio_segments(test_mfcc_features)\n",
    "\n",
    "X_test = np.array(padded_test_mfcc_features)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Test the model on the test dataset and get predictions\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Improved output formatting with confidence levels\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    confidence = predictions[i][0]  # Confidence level for each prediction\n",
    "    if y_pred[i][0] == 1:\n",
    "        print(f\"Audio segment {i+1}: Bee detected with confidence {confidence:.2f}\")\n",
    "    else:\n",
    "        print(f\"Audio segment {i+1}: No bee detected with confidence {1-confidence:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
